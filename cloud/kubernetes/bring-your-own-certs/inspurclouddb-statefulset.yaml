# This config file defines a ZNBaseDB StatefulSet that uses certificates
# created outside of Kubernetes. You may want to use it if you want to use a
# different certificate authority from the one being used by Kubernetes or if
# your Kubernetes cluster doesn't fully support certificate-signing requests
# (e.g. as of July 2018, EKS doesn't work properly).
#
# To use this config file, first set up your certificates and load them into
# your Kubernetes cluster as Secrets using the commands below:
#
# mkdir certs
# mkdir my-safe-directory
# znbase cert create-ca --certs-dir=certs --ca-key=my-safe-directory/ca.key
# znbase cert create-client root --certs-dir=certs --ca-key=my-safe-directory/ca.key
# kubectl create secret generic znbasedb.client.root --from-file=certs
# znbase cert create-node --certs-dir=certs --ca-key=my-safe-directory/ca.key localhost 127.0.0.1 znbasedb-public znbasedb-public.default znbasedb-public.default.svc.cluster.local *.znbasedb *.znbasedb.default *.znbasedb.default.svc.cluster.local
# kubectl create secret generic znbasedb.node --from-file=certs
# kubectl create -f bring-your-own-certs-statefulset.yaml
# kubectl exec -it znbasedb-0 -- /znbase/znbase init --certs-dir=/znbase/znbase-certs
apiVersion: v1
kind: ServiceAccount
metadata:
  name: znbasedb
  labels:
    app: znbasedb
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: Role
metadata:
  name: znbasedb
  labels:
    app: znbasedb
rules:
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: znbasedb
  labels:
    app: znbasedb
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: znbasedb
subjects:
- kind: ServiceAccount
  name: znbasedb
  namespace: default
---
apiVersion: v1
kind: Service
metadata:
  # This service is meant to be used by clients of the database. It exposes a ClusterIP that will
  # automatically load balance connections to the different database pods.
  name: znbasedb-public
  labels:
    app: znbasedb
spec:
  ports:
  # The main port, served by gRPC, serves Postgres-flavor SQL, internode
  # traffic and the cli.
  - port: 26257
    targetPort: 26257
    name: grpc
  # The secondary port serves the UI as well as health and debug endpoints.
  - port: 8080
    targetPort: 8080
    name: http
  selector:
    app: znbasedb
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: znbasedb
  labels:
    app: znbasedb
  annotations:
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
    prometheus.io/scrape: "true"
    prometheus.io/path: "_status/vars"
    prometheus.io/port: "8080"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other ZNBaseDB pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  clusterIP: None
  selector:
    app: znbasedb
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: znbasedb-budget
  labels:
    app: znbasedb
spec:
  selector:
    matchLabels:
      app: znbasedb
  maxUnavailable: 1
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: znbasedb
spec:
  serviceName: "znbasedb"
  replicas: 3
  template:
    metadata:
      labels:
        app: znbasedb
    spec:
      serviceAccountName: znbasedb
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - znbasedb
              topologyKey: kubernetes.io/hostname
      containers:
      - name: znbasedb
        image: znbasedb/znbase:v2.1.6
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        livenessProbe:
          httpGet:
            path: "/health"
            port: http
            scheme: HTTPS
          initialDelaySeconds: 30
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: "/health?ready=1"
            port: http
            scheme: HTTPS
          initialDelaySeconds: 10
          periodSeconds: 5
          failureThreshold: 2
        volumeMounts:
        - name: datadir
          mountPath: /znbase/znbase-data
        - name: certs
          mountPath: /znbase/znbase-certs
        env:
        - name: ZNBASE_CHANNEL
          value: kubernetes-secure
        command:
          - "/bin/bash"
          - "-ecx"
          # The use of qualified `hostname -f` is crucial:
          # Other nodes aren't able to look up the unqualified hostname.
          - "exec /znbase/znbase start --logtostderr --certs-dir /znbase/znbase-certs --advertise-host $(hostname -f) --http-addr 0.0.0.0 --join znbasedb-0.znbasedb,znbasedb-1.znbasedb,znbasedb-2.znbasedb --cache 25% --max-sql-memory 25%"
      # No pre-stop hook is required, a SIGTERM plus some time is all that's
      # needed for graceful shutdown of a node.
      terminationGracePeriodSeconds: 60
      volumes:
      - name: datadir
        persistentVolumeClaim:
          claimName: datadir
      - name: certs
        secret:
          secretName: znbasedb.node
          defaultMode: 256
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 100Gi
