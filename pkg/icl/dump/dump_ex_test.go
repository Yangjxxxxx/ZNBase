package dump_test

import (
	"bytes"
	"context"
	gosql "database/sql"
	"fmt"
	"hash/crc32"
	"io"
	"io/ioutil"
	"math/rand"
	"net/url"
	"os"
	"path/filepath"
	"strings"
	"sync/atomic"
	"testing"
	"time"

	"github.com/pkg/errors"
	"github.com/znbasedb/znbase-go/zbdb"
	"github.com/znbasedb/znbase/pkg/base"
	"github.com/znbasedb/znbase/pkg/icl/dump"
	"github.com/znbasedb/znbase/pkg/icl/utilicl/sampledataicl"
	"github.com/znbasedb/znbase/pkg/internal/client"
	"github.com/znbasedb/znbase/pkg/keys"
	"github.com/znbasedb/znbase/pkg/kv"
	"github.com/znbasedb/znbase/pkg/roachpb"
	"github.com/znbasedb/znbase/pkg/sql/sem/tree"
	"github.com/znbasedb/znbase/pkg/sql/stats"
	"github.com/znbasedb/znbase/pkg/testutils"
	"github.com/znbasedb/znbase/pkg/testutils/serverutils"
	"github.com/znbasedb/znbase/pkg/testutils/sqlutils"
	"github.com/znbasedb/znbase/pkg/testutils/testcluster"
	"github.com/znbasedb/znbase/pkg/util/leaktest"
	"github.com/znbasedb/znbase/pkg/util/protoutil"
	"github.com/znbasedb/znbase/pkg/util/randutil"
	"github.com/znbasedb/znbase/pkg/util/retry"
	"github.com/znbasedb/znbase/pkg/util/stop"
	"github.com/znbasedb/znbase/pkg/workload/bank"
	"golang.org/x/sync/errgroup"
)

func checksumBankPayload(t *testing.T, sqlDB *sqlutils.SQLRunner) uint32 {
	crc := crc32.New(crc32.MakeTable(crc32.Castagnoli))
	rows := sqlDB.Query(t, `SELECT id, balance, payload FROM data.bank`)
	defer rows.Close()
	var id, balance int
	var payload []byte
	for rows.Next() {
		if err := rows.Scan(&id, &balance, &payload); err != nil {
			t.Fatal(err)
		}
		if _, err := crc.Write(payload); err != nil {
			t.Fatal(err)
		}
	}
	if err := rows.Err(); err != nil {
		t.Fatal(err)
	}
	return crc.Sum32()
}

func TestDumpLoadIncremental(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 10
	const numBackups = 4
	windowSize := int(numAccounts / 3)

	_, tc, sqlDB, dir, cleanupFn := dumpLoadTestSetup(t, singleNode, 0, initNone)
	defer cleanupFn()
	args := base.TestServerArgs{ExternalIODir: dir}
	rng, _ := randutil.NewPseudoRand()

	var backupDirs []string
	var checksums []uint32
	{
		for backupNum := 0; backupNum < numBackups; backupNum++ {
			// In the following, windowSize is `w` and offset is `o`. The first
			// mutation creates accounts with id [w,3w). Every mutation after
			// that deletes everything less than o, leaves [o, o+w) unchanged,
			// mutates [o+w,o+2w), and inserts [o+2w,o+3w).
			offset := windowSize * backupNum
			var buf bytes.Buffer
			fmt.Fprintf(&buf, `DELETE FROM data.bank WHERE id < %d; `, offset)
			buf.WriteString(`UPSERT INTO data.bank VALUES `)
			for j := 0; j < windowSize*2; j++ {
				if j != 0 {
					buf.WriteRune(',')
				}
				id := offset + windowSize + j
				payload := randutil.RandBytes(rng, dumpLoadRowPayloadSize)
				fmt.Fprintf(&buf, `(%d, %d, '%s')`, id, backupNum, payload)
			}
			sqlDB.Exec(t, buf.String())

			checksums = append(checksums, checksumBankPayload(t, sqlDB))

			backupDir := fmt.Sprintf("nodelocal:///%d", backupNum)
			var from string
			if backupNum > 0 {
				from = fmt.Sprintf(` INCREMENTAL FROM %s`, strings.Join(backupDirs, `,`))
			}
			sqlDB.Exec(t, fmt.Sprintf(`DUMP TABLE data.bank TO SST '%s' %s`, backupDir, from))

			backupDirs = append(backupDirs, fmt.Sprintf(`'%s'`, backupDir))
		}

		// Test a regression in LOAD where the batch end key was not
		// being set correctly in Import: make an incremental backup such that
		// the greatest key in the diff is less than the previous backups.
		sqlDB.Exec(t, `INSERT INTO data.bank VALUES (0, -1, 'final')`)
		checksums = append(checksums, checksumBankPayload(t, sqlDB))
		sqlDB.Exec(t, fmt.Sprintf(`DUMP TABLE data.bank TO SST '%s' %s`,
			"nodelocal:///final", fmt.Sprintf(` INCREMENTAL FROM %s`, strings.Join(backupDirs, `,`)),
		))
		backupDirs = append(backupDirs, `'nodelocal:///final'`)
	}

	// Start a new cluster to restore into.
	{
		restoreTC := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer restoreTC.Stopper().Stop(context.TODO())
		sqlDBRestore := sqlutils.MakeSQLRunner(restoreTC.Conns[0])

		sqlDBRestore.Exec(t, `CREATE DATABASE data`)
		sqlDBRestore.Exec(t, `CREATE TABLE data.bank (id INT PRIMARY KEY)`)
		// This "data.bank" table isn't actually the same table as the backup at all
		// so we should not allow using a backup of the other in incremental. We
		// usually compare IDs, but those are only meaningful in the context of a
		// single cluster, so we also need to ensure the previous backup was indeed
		// generated by the same cluster.

		sqlDBRestore.ExpectErr(
			t, fmt.Sprintf("belongs to cluster %s", tc.Servers[0].ClusterID()),
			`DUMP TABLE data.bank TO SST $1 INCREMENTAL FROM $2`,
			"nodelocal:///some-other-table", "nodelocal:///0",
		)

		for i := len(backupDirs); i > 0; i-- {
			sqlDBRestore.Exec(t, `DROP TABLE IF EXISTS data.bank`)
			from := strings.Join(backupDirs[:i], `,`)
			sqlDBRestore.Exec(t, fmt.Sprintf(`LOAD TABLE data.bank FROM %s`, from))

			testutils.SucceedsSoon(t, func() error {
				checksum := checksumBankPayload(t, sqlDBRestore)
				if checksum != checksums[i-1] {
					return errors.Errorf("checksum mismatch at index %d: got %d expected %d",
						i-1, checksum, checksums[i])
				}
				return nil
			})

		}
	}
}
func TestBackUPAndRestoreEncryptionAndCompression(t *testing.T) {
	defer leaktest.AfterTest(t)()
	const numAccounts = 1000

	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	const dir = "nodelocal:///"

	dumpTableEncrypt, dumpTableCompression, dumpTableEncryptCompression := filepath.Join(dir, "dumpTableEncrypt"), filepath.Join(dir, "dumpTableCompression"), filepath.Join(dir, "dumpTableEncryptCompression")

	sqlDB.Exec(t, `CREATE TABLE data.t1(c1 int,name string)`)
	sqlDB.Exec(t, `INSERT INTO data.t1 VALUES (1,'xiaoming'),(2,'xiaohong')`)

	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP TABLE data.public.t1 TO SST $1  WITH encryption_passphrase = '123'`),
		dumpTableEncrypt,
	)
	sqlDB.ExpectErr(t,
		`pq: option "encryption_passphrase" requires a value`,
		fmt.Sprintf(`DUMP TABLE data.public.t1 TO SST $1  WITH encryption_passphrase`),
		dumpTableEncrypt,
	)

	sqlDB.ExpectErr(t,
		`pq: unsupported compression codec bzip`,
		`DUMP TABLE data.public.t1 TO SST $1  WITH compression = bzip`,
		dumpTableCompression,
	)

	sqlDB.Exec(t,
		`DUMP TABLE data.public.t1 TO SST $1  WITH compression = gzip`,
		dumpTableCompression,
	)

	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP TABLE data.public.t1 TO SST $1  WITH encryption_passphrase = '123',compression = gzip`),
		dumpTableEncryptCompression,
	)

	expected := sqlDB.QueryStr(
		t, fmt.Sprintf(`SELECT * FROM data.t1`),
	)

	//测试错误密码
	sqlDB.Exec(t, `CREATE DATABASE db1`)
	sqlDB.ExpectErr(t,
		`pq: failed to read backup descriptor: cipher: message authentication failed`,
		`LOAD TABLE data.t1 FROM $1 WITH encryption_passphrase = 'abcdefg',into_db = 'db1'`,
		dumpTableEncrypt,
	)
	//测试不输入密码
	sqlDB.ExpectErr(t,
		`pq: failed to read backup descriptor: file appears encrypted -- try specifying "encryption_passphrase": proto: wrong wireType = 5 for field EntryCounts`,
		`LOAD TABLE data.t1 FROM $1 WITH into_db = 'db1'`,
		dumpTableEncrypt,
	)
	//测试正确导入加密文件
	sqlDB.Exec(t,
		`LOAD TABLE data.t1 FROM $1 WITH encryption_passphrase = '123',into_db = 'db1'`,
		dumpTableEncrypt,
	)
	sqlDB.CheckQueryResults(t, `SELECT * FROM db1.t1`, expected)

	sqlDB.Exec(t, `CREATE DATABASE db2`)
	//测试正确导入压缩文件
	sqlDB.Exec(t,
		`LOAD TABLE data.t1 FROM $1 WITH into_db = 'db2'`,
		dumpTableCompression,
	)
	sqlDB.CheckQueryResults(t, `SELECT * FROM db2.t1`, expected)

	//测试导入压缩加密文件
	sqlDB.Exec(t, `CREATE DATABASE db3`)
	sqlDB.Exec(t,
		`LOAD TABLE data.t1 FROM $1 WITH encryption_passphrase = '123',into_db = 'db3'`,
		dumpTableEncryptCompression,
	)
	sqlDB.CheckQueryResults(t, `SELECT * FROM db3.t1`, expected)
	//测试整库的加密压缩
	t.Run("test database encrypt and compression", func(t *testing.T) {
		dbEncrypt, dbCompression := filepath.Join(dir, "dbEncrypt"), filepath.Join(dir, "dbCompression")
		sqlDB.Exec(t,
			fmt.Sprintf(`DUMP DATABASE db1 TO SST $1 WITH encryption_passphrase = '123'`),
			dbEncrypt,
		)
		sqlDB.Exec(t, `DROP DATABASE db1 CASCADE`)
		sqlDB.Exec(t,
			fmt.Sprintf(`LOAD DATABASE db1 FROM $1 WITH encryption_passphrase = '123'`),
			dbEncrypt)
		sqlDB.CheckQueryResults(t, `SELECT * FROM db1.t1`, expected)

		sqlDB.Exec(t,
			fmt.Sprintf(`DUMP DATABASE db1 TO SST $1 WITH compression = 'gzip'`),
			dbCompression,
		)
		sqlDB.Exec(t, `DROP DATABASE db1 CASCADE`)
		sqlDB.Exec(t,
			fmt.Sprintf(`LOAD DATABASE db1 FROM $1`),
			dbCompression)
		sqlDB.CheckQueryResults(t, `SELECT * FROM db1.t1`, expected)
	})

	//测试模式的加密压缩
	t.Run("test schema encrypt and compression", func(t *testing.T) {
		t.Skip("测试用例出现几率性不过，未知原因，暂时跳过，可能与权限或内部执行器有关")
		sqlDB.Exec(t, `CREATE DATABASE db4`)
		sqlDB.Exec(t, `CREATE SCHEMA db4.s1`)
		sqlDB.Exec(t, `CREATE TABLE db4.s1.t1(c1 int,name string)`)
		sqlDB.Exec(t, `INSERT INTO db4.s1.t1 VALUES (1,'xiaoming'),(2,'xiaohong')`)
		scEncrypt, scCompression := filepath.Join(dir, "scEncrypt"), filepath.Join(dir, "scCompression")
		sqlDB.Exec(t,
			fmt.Sprintf(`DUMP SCHEMA db4.s1 TO SST $1 WITH encryption_passphrase = '123'`),
			scEncrypt,
		)
		sqlDB.Exec(t, `DROP SCHEMA db4.s1 CASCADE`)
		sqlDB.Exec(t,
			fmt.Sprintf(`LOAD SCHEMA db4.s1 FROM $1 WITH encryption_passphrase = '123'`),
			scEncrypt)
		sqlDB.CheckQueryResults(t, `SELECT * FROM db4.s1.t1`, expected)

		sqlDB.Exec(t,
			fmt.Sprintf(`DUMP SCHEMA db4.s1 TO SST $1 WITH compression = 'gzip'`),
			scCompression,
		)
		sqlDB.Exec(t, `DROP SCHEMA db4.s1 CASCADE`)
		sqlDB.Exec(t,
			fmt.Sprintf(`LOAD SCHEMA db4.s1 FROM $1`),
			scCompression)
		sqlDB.CheckQueryResults(t, `SELECT * FROM db4.s1.t1`, expected)
	})
}
func TestDumpAndLoadEncryptionAndCompression(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1000

	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	const dir = "nodelocal:///"

	dumpTableEncrypt, dumpTableCompression, dumpTableEncryptCompression := filepath.Join(dir, "dumpTableEncrypt"), filepath.Join(dir, "dumpTableCompression"), filepath.Join(dir, "dumpTableEncryptCompression")

	sqlDB.Exec(t, `CREATE TABLE data.t1(c1 int,name string)`)
	sqlDB.Exec(t, `INSERT INTO data.t1 VALUES (1,'xiaoming'),(2,'xiaohong')`)

	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP TO CSV $1 FROM TABLE data.public.t1 WITH encryption_passphrase = '123'`),
		dumpTableEncrypt,
	)
	sqlDB.ExpectErr(t,
		`pq: option "encryption_passphrase" requires a value`,
		fmt.Sprintf(`DUMP TO CSV $1 FROM TABLE data.public.t1 WITH encryption_passphrase`),
		dumpTableEncrypt,
	)

	sqlDB.ExpectErr(t,
		`pq: unsupported compression codec bzip`,
		`DUMP TO CSV $1 FROM TABLE data.public.t1 WITH compression = bzip`,
		dumpTableCompression,
	)

	sqlDB.Exec(t,
		`DUMP TO CSV $1 FROM TABLE data.public.t1 WITH compression = gzip`,
		dumpTableCompression,
	)

	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP TO CSV $1 FROM TABLE data.public.t1 WITH encryption_passphrase = '123',compression = gzip`),
		dumpTableEncryptCompression,
	)

	dumpTableEncrypt, dumpTableCompression, dumpTableEncryptCompression = filepath.Join(dumpTableEncrypt, "n1.0.csv"), filepath.Join(dumpTableCompression, "n1.0.csv.gz"), filepath.Join(dumpTableEncryptCompression, "n1.0.csv.gz")

	expected := sqlDB.QueryStr(
		t, fmt.Sprintf(`SELECT * FROM data.t1`),
	)

	//测试错误密码
	sqlDB.ExpectErr(t,
		`pq: nodelocal:/dumpTableEncrypt/n1.0.csv: cipher: message authentication failed`,
		`LOAD TABLE data.t2 (c1 int ,name string) CSV DATA ($1) WITH encryption_passphrase = 'abcdefg'`,
		dumpTableEncrypt,
	)
	//测试不输入密码
	sqlDB.ExpectErr(t,
		`pq: nodelocal:/dumpTableEncrypt/n1.0.csv: file appears encrypted -- try specifying "encryption_passphrase"`,
		`LOAD TABLE data.t2 (c1 int ,name string) CSV DATA ($1)`,
		dumpTableEncrypt,
	)
	//测试正确导入加密文件
	sqlDB.Exec(t,
		`LOAD TABLE data.t2 (c1 int ,name string) CSV DATA ($1) WITH encryption_passphrase = '123'`,
		dumpTableEncrypt,
	)
	sqlDB.CheckQueryResults(t, `SELECT * FROM data.t2`, expected)

	//测试导入压缩文件指定错误的格式
	sqlDB.ExpectErr(t,
		`pq: nodelocal:/dumpTableCompression/n1.0.csv.gz: row 1: reading CSV record: bzip2 data invalid: bad magic value`,
		`LOAD TABLE data.t3 (c1 int ,name string) CSV DATA ($1) WITH decompress = bzip`,
		dumpTableCompression,
	)
	//测试导入不存在的压缩格式
	sqlDB.ExpectErr(t,
		`pq: unsupported compression value: "zip"`,
		`LOAD TABLE data.t3 (c1 int ,name string) CSV DATA ($1) WITH decompress = zip`,
		dumpTableCompression,
	)
	//测试正确导入压缩文件
	sqlDB.Exec(t,
		`LOAD TABLE data.t3 (c1 int ,name string) CSV DATA ($1)`,
		dumpTableCompression,
	)
	sqlDB.CheckQueryResults(t, `SELECT * FROM data.t3`, expected)

	//测试导入压缩加密文件
	sqlDB.Exec(t,
		`LOAD TABLE data.t4 (c1 int ,name string) CSV DATA ($1) WITH encryption_passphrase = '123'`,
		dumpTableEncryptCompression,
	)
	sqlDB.CheckQueryResults(t, `SELECT * FROM data.t4`, expected)
}

func TestDumpAndLoadDatabase(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1000

	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	const dir = "nodelocal:///"

	sqlDB.Exec(t, `CREATE TABLE data.t1(c1 int,name string)`)
	sqlDB.Exec(t, `INSERT INTO data.t1 VALUES (1,'xiaoming'),(2,'xiaohong')`)

	expected := sqlDB.QueryStr(
		t, fmt.Sprintf(`SELECT * FROM data.t1`),
	)

	dumpdatabase := filepath.Join(dir, "dumpdatabase")
	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP TO CSV $1 FROM database data`),
		dumpdatabase,
	)
	CreateSql := dumpdatabase + "/data.sql"
	sqlDB.Exec(t, `drop DATABASE data cascade`)
	sqlDB.Exec(t,
		`LOAD DATABASE data CREATE USING $1 CSV data ($2)`,
		CreateSql, dumpdatabase,
	)
	sqlDB.CheckQueryResults(t, `SELECT * FROM data.t1`, expected)

	dumpdatabaseTxt := filepath.Join(dir, "dumpdatabasetxt")
	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP TO TXT $1 FROM database data`),
		dumpdatabaseTxt,
	)
	CreateSql = dumpdatabaseTxt + "/data.sql"
	sqlDB.Exec(t, `drop DATABASE data cascade`)

	sqlDB.Exec(t,
		`LOAD DATABASE data CREATE USING $1 TXT data ($2)`,
		CreateSql, dumpdatabaseTxt,
	)
	sqlDB.CheckQueryResults(t, `SELECT * FROM data.t1`, expected)
}

func TestDumpAndLoadSchema(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1000

	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	const dir = "nodelocal:///"

	sqlDB.Exec(t, `CREATE schema s1`)
	sqlDB.Exec(t, `CREATE TABLE data.s1.t1(c1 int,name string)`)
	sqlDB.Exec(t, `INSERT INTO data.s1.t1 VALUES (1,'xiaoming'),(2,'xiaohong')`)

	expected := sqlDB.QueryStr(
		t, fmt.Sprintf(`SELECT * FROM data.s1.t1`),
	)

	dumpschema := filepath.Join(dir, "schema")
	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP TO CSV $1 FROM schema data.s1`),
		dumpschema,
	)
	CreateSql := dumpschema + "/s1.sql"
	sqlDB.Exec(t, `drop schema data.s1 cascade`)

	sqlDB.Exec(t,
		`LOAD schema data.s1 CREATE USING $1 CSV data ($2)`,
		CreateSql, dumpschema,
	)
	sqlDB.CheckQueryResults(t, `SELECT * FROM data.s1.t1`, expected)

	dumpschemaTxt := filepath.Join(dir, "schematxt")
	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP TO TXT $1 FROM schema data.s1`),
		dumpschemaTxt,
	)

	sqlDB.Exec(t, `drop schema data.s1 cascade`)
	CreateSql = dumpschemaTxt + "/s1.sql"
	sqlDB.Exec(t,
		`LOAD schema data.s1 CREATE USING $1 TXT data ($2)`,
		CreateSql, dumpschemaTxt,
	)
	sqlDB.CheckQueryResults(t, `SELECT * FROM data.s1.t1`, expected)
}

// a bg worker is intended to write to the bank table concurrent with other
// operations (writes, backups, restores), mutating the payload on rows-maxID.
// it notified the `wake` channel (to allow ensuring bg activity has occurred)
// and can be informed when errors are allowable (e.g. when the bank table is
// unavailable between a drop and restore) via the atomic "bool" allowErrors.
func startBackgroundWrites(
	stopper *stop.Stopper, sqlDB *gosql.DB, maxID int, wake chan<- struct{}, allowErrors *int32,
) error {
	rng, _ := randutil.NewPseudoRand()

	for {
		select {
		case <-stopper.ShouldQuiesce():
			return nil // All done.
		default:
			// Keep going.
		}

		id := rand.Intn(maxID)
		payload := randutil.RandBytes(rng, dumpLoadRowPayloadSize)

		updateFn := func() error {
			select {
			case <-stopper.ShouldQuiesce():
				return nil // All done.
			default:
				// Keep going.
			}
			if atomic.LoadInt32(allowErrors) == -1 {
				return nil
			}
			_, err := sqlDB.Exec(`UPDATE data.bank SET payload = $1 WHERE id = $2`, payload, id)
			if atomic.LoadInt32(allowErrors) == 1 {
				return nil
			}
			return err
		}
		if err := retry.ForDuration(testutils.DefaultSucceedsSoonDuration, updateFn); err != nil {
			return err
		}
		select {
		case wake <- struct{}{}:
		default:
		}
	}
}

func TestDumpLoadWithConcurrentWrites(t *testing.T) {
	defer leaktest.AfterTest(t)()
	const rows = 10
	const numBackgroundTasks = multiNode

	_, tc, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, multiNode, rows, initNone)
	defer cleanupFn()

	bgActivity := make(chan struct{})
	// allowErrors is used as an atomic bool to tell bg workers when to allow
	// errors, between dropping and restoring the table they are using.
	var allowErrors int32
	for task := 0; task < numBackgroundTasks; task++ {
		taskNum := task
		tc.Stopper().RunWorker(context.TODO(), func(context.Context) {
			conn := tc.Conns[taskNum%len(tc.Conns)]
			// Use different sql gateways to make sure leasing is right.
			if err := startBackgroundWrites(tc.Stopper(), conn, rows, bgActivity, &allowErrors); err != nil {
				t.Error(err)
			}
		})
	}

	// Use the data.bank table as a key (id), value (balance) table with a
	// payload.The background tasks are mutating the table concurrently while we
	// backup and restore.
	<-bgActivity

	// Set, break, then reset the id=balance invariant -- while doing concurrent
	// writes -- to get multiple MVCC revisions as well as txn conflicts.
	sqlDB.Exec(t, `UPDATE data.bank SET balance = id`)
	<-bgActivity
	sqlDB.Exec(t, `UPDATE data.bank SET balance = -1`)
	<-bgActivity
	sqlDB.Exec(t, `UPDATE data.bank SET balance = id`)
	<-bgActivity

	// Backup DB while concurrent writes continue.
	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1`, localFoo)
	// Drop the table and restore from backup and check our invariant.
	//TODO 在进行并发写的同时去DROP TABLE 会出现卡住的场景，对于本测试没有逻辑上的影响。因此在DROP TABLE时关闭并发写
	atomic.StoreInt32(&allowErrors, -1)
	sqlDB.Exec(t, `DROP TABLE data.bank`)
	atomic.StoreInt32(&allowErrors, 1)
	sqlDB.Exec(t, `LOAD TABLE data.bank FROM $1`, localFoo)
	atomic.StoreInt32(&allowErrors, 0)

	bad := sqlDB.QueryStr(t, `SELECT id, balance, payload FROM data.bank WHERE id != balance`)
	for _, r := range bad {
		t.Errorf("bad row ID %s = bal %s (payload: %q)", r[0], r[1], r[2])
	}
}

func TestConcurrentDumpLoads(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 10
	const concurrency, numIterations = 2, 3
	ctx, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, multiNode, numAccounts, initNone)
	defer cleanupFn()

	g, gCtx := errgroup.WithContext(ctx)
	for i := 0; i < concurrency; i++ {
		table := fmt.Sprintf("bank_%d", i)
		sqlDB.Exec(t, fmt.Sprintf(
			`CREATE TABLE data.%s AS (SELECT * FROM data.bank WHERE id > %d ORDER BY id)`,
			table, i,
		))
		g.Go(func() error {
			for j := 0; j < numIterations; j++ {
				dbName := fmt.Sprintf("%s_%d", table, j)
				backupDir := fmt.Sprintf("nodelocal:///%s", dbName)
				backupQ := fmt.Sprintf(`DUMP data.%s TO SST $1`, table)
				if _, err := sqlDB.DB.ExecContext(gCtx, backupQ, backupDir); err != nil {
					return err
				}
				if _, err := sqlDB.DB.ExecContext(gCtx, fmt.Sprintf(`CREATE DATABASE %s`, dbName)); err != nil {
					return err
				}
				restoreQ := fmt.Sprintf(`LOAD TABLE data.%s FROM $1 WITH OPTIONS ('into_db'='%s')`, table, dbName)
				if _, err := sqlDB.DB.ExecContext(gCtx, restoreQ, backupDir); err != nil {
					return err
				}
			}
			return nil
		})
	}
	if err := g.Wait(); err != nil {
		t.Fatalf("%+v", err)
	}

	for i := 0; i < concurrency; i++ {
		orig := sqlDB.QueryStr(t, `SELECT * FROM data.bank WHERE id > $1 ORDER BY id`, i)
		for j := 0; j < numIterations; j++ {
			selectQ := fmt.Sprintf(`SELECT * FROM bank_%d_%d.bank_%d ORDER BY id`, i, j, i)
			sqlDB.CheckQueryResults(t, selectQ, orig)
		}
	}
}

func TestDumpAsOfSystemTime(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1000

	ctx, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	var beforeTs, equalTs string
	var rowCount int

	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&beforeTs)

	err := zbdb.ExecuteTx(ctx, sqlDB.DB.(*gosql.DB), nil /* txopts */, func(tx *gosql.Tx) error {
		_, err := tx.Exec(`DELETE FROM data.bank WHERE id % 4 = 1`)
		if err != nil {
			return err
		}
		return tx.QueryRow(`SELECT cluster_logical_timestamp()`).Scan(&equalTs)
	})
	if err != nil {
		t.Fatal(err)
	}

	sqlDB.QueryRow(t, `SELECT count(*) FROM data.bank`).Scan(&rowCount)
	if expected := numAccounts * 3 / 4; rowCount != expected {
		t.Fatalf("expected %d rows but found %d", expected, rowCount)
	}

	beforeDir := filepath.Join(localFoo, `beforeTs`)
	sqlDB.Exec(t, fmt.Sprintf(`DUMP DATABASE data TO SST '%s' AS OF SYSTEM TIME %s`, beforeDir, beforeTs))
	equalDir := filepath.Join(localFoo, `equalTs`)
	sqlDB.Exec(t, fmt.Sprintf(`DUMP DATABASE data TO SST '%s' AS OF SYSTEM TIME %s`, equalDir, equalTs))

	sqlDB.Exec(t, `DROP TABLE data.bank`)
	sqlDB.Exec(t, `LOAD TABLE data.bank FROM $1`, beforeDir)
	sqlDB.QueryRow(t, `SELECT count(*) FROM data.bank`).Scan(&rowCount)
	if expected := numAccounts; rowCount != expected {
		t.Fatalf("expected %d rows but found %d", expected, rowCount)
	}

	sqlDB.Exec(t, `DROP TABLE data.bank`)
	sqlDB.Exec(t, `LOAD TABLE data.bank FROM $1`, equalDir)
	sqlDB.QueryRow(t, `SELECT count(*) FROM data.bank`).Scan(&rowCount)
	if expected := numAccounts * 3 / 4; rowCount != expected {
		t.Fatalf("expected %d rows but found %d", expected, rowCount)
	}
}

func TestLoadAsOfSystemTime(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 10
	ctx, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	const dir = "nodelocal:///"

	ts := make([]string, 9)

	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[0])

	sqlDB.Exec(t, `UPDATE data.bank SET balance = 1`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[1])

	// Change the data in the tabe.
	sqlDB.Exec(t, `CREATE TABLE data.teller (id INT PRIMARY KEY, name STRING)`)
	sqlDB.Exec(t, `INSERT INTO data.teller VALUES (1, 'alice'), (7, 'bob'), (3, 'eve')`)

	err := zbdb.ExecuteTx(ctx, sqlDB.DB.(*gosql.DB), nil /* txopts */, func(tx *gosql.Tx) error {
		_, err := tx.Exec(`UPDATE data.bank SET balance = 2`)
		if err != nil {
			return err
		}
		return tx.QueryRow(`SELECT cluster_logical_timestamp()`).Scan(&ts[2])
	})
	if err != nil {
		t.Fatal(err)
	}

	fullBackup, latestBackup := filepath.Join(dir, "full"), filepath.Join(dir, "latest")
	incBackup, incLatestBackup := filepath.Join(dir, "inc"), filepath.Join(dir, "inc-latest")
	inc2Backup, inc2LatestBackup := incBackup+".2", incLatestBackup+".2"

	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP DATABASE data TO SST $1 AS OF SYSTEM TIME %s WITH revision_history`, ts[2]),
		fullBackup,
	)
	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP DATABASE data TO SST $1 AS OF SYSTEM TIME %s`, ts[2]),
		latestBackup,
	)

	fullTableBackup := filepath.Join(dir, "tbl")
	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP data.bank TO SST $1 AS OF SYSTEM TIME %s WITH revision_history`, ts[2]),
		fullTableBackup,
	)

	sqlDB.Exec(t, `UPDATE data.bank SET balance = 3`)

	// Create a table in some other DB -- this won't be in this backup (yet).
	sqlDB.Exec(t, `CREATE DATABASE other`)
	sqlDB.Exec(t, `CREATE TABLE other.sometable (id INT PRIMARY KEY, somevalue INT)`)
	sqlDB.Exec(t, `INSERT INTO other.sometable VALUES (1, 2), (7, 5), (3, 3)`)

	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[3])

	sqlDB.Exec(t, `DELETE FROM data.bank WHERE id >= $1 / 2`, numAccounts)
	sqlDB.Exec(t, `ALTER TABLE other.sometable RENAME TO data.sometable`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[4])

	sqlDB.Exec(t, `INSERT INTO data.sometable VALUES (2, 2), (4, 5), (6, 3)`)
	sqlDB.Exec(t, `ALTER TABLE data.bank ADD COLUMN points_balance INT DEFAULT 50`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[5])

	sqlDB.Exec(t, `TRUNCATE TABLE data.bank`)
	sqlDB.Exec(t, `TRUNCATE TABLE data.bank`)
	sqlDB.Exec(t, `TRUNCATE TABLE data.bank`)
	sqlDB.Exec(t, `ALTER TABLE data.sometable RENAME TO other.sometable`)
	sqlDB.Exec(t, `CREATE INDEX ON data.teller (name)`)
	sqlDB.Exec(t, `INSERT INTO data.bank VALUES (2, 2), (4, 4)`)
	sqlDB.Exec(t, `INSERT INTO data.teller VALUES (2, 'craig')`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[6])

	sqlDB.Exec(t, `TRUNCATE TABLE data.bank`)
	sqlDB.Exec(t, `INSERT INTO data.bank VALUES (2, 2), (4, 4)`)
	sqlDB.Exec(t, `DROP TABLE other.sometable`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[7])

	sqlDB.Exec(t, `UPSERT INTO data.bank (id, balance)
	           SELECT i, 4 FROM generate_series(0, $1 - 1) AS g(i)`, numAccounts)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[8])

	//原因在函数startDumpJob的141行，由于sometable的parentID是schema，它直接找到databaseID.
	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP DATABASE data TO SST $1 AS OF SYSTEM TIME %s INCREMENTAL FROM $2 WITH revision_history`, ts[5]),
		incBackup, fullBackup,
	)

	sqlDB.Exec(t,
		`DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2, $3 WITH revision_history`,
		inc2Backup, fullBackup, incBackup,
	)

	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP DATABASE data TO SST $1 AS OF SYSTEM TIME %s INCREMENTAL FROM $2`, ts[5]),
		incLatestBackup, latestBackup,
	)
	sqlDB.Exec(t,
		`DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2, $3`,
		inc2LatestBackup, latestBackup, incLatestBackup,
	)

	incTableBackup := filepath.Join(dir, "inctbl")
	sqlDB.Exec(t,
		`DUMP data.bank TO SST $1 INCREMENTAL FROM $2 WITH revision_history`,
		incTableBackup, fullTableBackup,
	)

	var after string
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&after)

	for i, timestamp := range ts {
		name := fmt.Sprintf("ts%d", i)
		t.Run(name, func(t *testing.T) {
			sqlDB = sqlutils.MakeSQLRunner(sqlDB.DB)
			// Create new DBs into which we'll restore our copies without conflicting
			// with the existing, original table.
			sqlDB.Exec(t, fmt.Sprintf(`CREATE DATABASE %s`, name))
			sqlDB.Exec(t, fmt.Sprintf(`CREATE DATABASE %stbl`, name))
			// Restore the bank table from the full DB MVCC backup to time x, into a
			// separate DB so that we can later compare it to the original table via
			// time-travel.
			sqlDB.Exec(t,
				fmt.Sprintf(
					`LOAD TABLE data.bank FROM $1, $2, $3 AS OF SYSTEM TIME %s WITH into_db='%s'`,
					timestamp, name,
				),
				fullBackup, incBackup, inc2Backup,
			)
			// Similarly restore the since-table backup -- since full DB and single table
			// backups sometimes behave differently.
			sqlDB.Exec(t,
				fmt.Sprintf(
					`LOAD TABLE data.bank FROM $1, $2 AS OF SYSTEM TIME %s WITH into_db='%stbl'`,
					timestamp, name,
				),
				fullTableBackup, incTableBackup,
			)

			// Use time-travel on the existing bank table to determine what LOAD
			// with AS OF should have produced.
			expected := sqlDB.QueryStr(
				t, fmt.Sprintf(`SELECT * FROM data.bank AS OF SYSTEM TIME %s ORDER BY id`, timestamp),
			)
			// Confirm reading (with no as-of) from the as-of restored table matches.
			sqlDB.CheckQueryResults(t, fmt.Sprintf(`SELECT * FROM %s.bank ORDER BY id`, name), expected)
			sqlDB.CheckQueryResults(t, fmt.Sprintf(`SELECT * FROM %stbl.bank ORDER BY id`, name), expected)

			// `sometable` moved in to data between after ts 3 and removed before 5.
			if i == 4 || i == 5 {
				sqlDB.Exec(t,
					fmt.Sprintf(
						`LOAD TABLE data.sometable FROM $1, $2 AS OF SYSTEM TIME %s WITH into_db='%s'`,
						timestamp, name,
					),
					fullBackup, incBackup,
				)
				sqlDB.CheckQueryResults(t,
					fmt.Sprintf(`SELECT * FROM %s.sometable ORDER BY id`, name),
					sqlDB.QueryStr(t, fmt.Sprintf(`SELECT * FROM data.sometable AS OF SYSTEM TIME %s ORDER BY id`, timestamp)),
				)
			}
			// teller was created after ts 2.
			if i > 2 {
				sqlDB.Exec(t,
					fmt.Sprintf(
						`LOAD TABLE data.teller FROM $1, $2, $3 AS OF SYSTEM TIME %s WITH into_db='%s'`,
						timestamp, name,
					),
					fullBackup, incBackup, inc2Backup,
				)
				sqlDB.CheckQueryResults(t,
					fmt.Sprintf(`SELECT * FROM %s.teller ORDER BY id`, name),
					sqlDB.QueryStr(t, fmt.Sprintf(`SELECT * FROM data.teller AS OF SYSTEM TIME %s ORDER BY id`, timestamp)),
				)
			}
		})
	}

	t.Run("latest", func(t *testing.T) {
		sqlDB = sqlutils.MakeSQLRunner(sqlDB.DB)
		// The "latest" backup didn't specify ALL mvcc values, so we can't restore
		// to times in the middle.
		sqlDB.Exec(t, `CREATE DATABASE err`)

		// fullBackup covers up to ts[2], inc to ts[5], inc2 to > ts[8].
		sqlDB.ExpectErr(
			t, "incompatible LOAD timestamp",
			fmt.Sprintf(`LOAD TABLE data.bank FROM $1 AS OF SYSTEM TIME %s WITH into_db='err'`, ts[3]),
			fullBackup,
		)

		for _, i := range ts {
			sqlDB.ExpectErr(
				t, "incompatible LOAD timestamp",
				fmt.Sprintf(`LOAD TABLE data.bank FROM $1 AS OF SYSTEM TIME %s WITH into_db='err'`, i),
				latestBackup,
			)

			sqlDB.ExpectErr(
				t, "incompatible LOAD timestamp",
				fmt.Sprintf(`LOAD TABLE data.bank FROM $1, $2, $3 AS OF SYSTEM TIME %s WITH into_db='err'`, i),
				latestBackup, incLatestBackup, inc2LatestBackup,
			)
		}

		sqlDB.ExpectErr(
			t, "incompatible LOAD timestamp",
			fmt.Sprintf(`LOAD TABLE data.bank FROM $1 AS OF SYSTEM TIME %s WITH into_db='err'`, after),
			latestBackup,
		)
	})
}

func TestOnlineLoadAsOfSystemTime(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 10
	ctx, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	const dir = "nodelocal:///"

	ts := make([]string, 9)

	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[0])

	sqlDB.Exec(t, `UPDATE data.bank SET balance = 1`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[1])

	// Change the data in the tabe.
	sqlDB.Exec(t, `CREATE TABLE data.teller (id INT PRIMARY KEY, name STRING)`)
	sqlDB.Exec(t, `INSERT INTO data.teller VALUES (1, 'alice'), (7, 'bob'), (3, 'eve')`)

	err := zbdb.ExecuteTx(ctx, sqlDB.DB.(*gosql.DB), nil /* txopts */, func(tx *gosql.Tx) error {
		_, err := tx.Exec(`UPDATE data.bank SET balance = 2`)
		if err != nil {
			return err
		}
		return tx.QueryRow(`SELECT cluster_logical_timestamp()`).Scan(&ts[2])
	})
	if err != nil {
		t.Fatal(err)
	}

	fullBackup, latestBackup := filepath.Join(dir, "full"), filepath.Join(dir, "latest")
	incBackup, incLatestBackup := filepath.Join(dir, "inc"), filepath.Join(dir, "inc-latest")
	inc2Backup, inc2LatestBackup := incBackup+".2", incLatestBackup+".2"

	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP DATABASE data TO SST $1  AS OF SYSTEM TIME %s WITH revision_history,online`, ts[2]),
		fullBackup,
	)
	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP DATABASE data TO SST $1 AS OF SYSTEM TIME %s WITH online`, ts[2]),
		latestBackup,
	)

	fullTableBackup := filepath.Join(dir, "tbl")
	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP data.bank TO SST $1 AS OF SYSTEM TIME %s WITH revision_history,online`, ts[2]),
		fullTableBackup,
	)

	sqlDB.Exec(t, `UPDATE data.bank SET balance = 3`)

	// Create a table in some other DB -- this won't be in this backup (yet).
	sqlDB.Exec(t, `CREATE DATABASE other`)
	sqlDB.Exec(t, `CREATE TABLE other.sometable (id INT PRIMARY KEY, somevalue INT)`)
	sqlDB.Exec(t, `INSERT INTO other.sometable VALUES (1, 2), (7, 5), (3, 3)`)

	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[3])

	sqlDB.Exec(t, `DELETE FROM data.bank WHERE id >= $1 / 2`, numAccounts)
	sqlDB.Exec(t, `ALTER TABLE other.sometable RENAME TO data.sometable`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[4])

	sqlDB.Exec(t, `INSERT INTO data.sometable VALUES (2, 2), (4, 5), (6, 3)`)
	sqlDB.Exec(t, `ALTER TABLE data.bank ADD COLUMN points_balance INT DEFAULT 50`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[5])

	sqlDB.Exec(t, `TRUNCATE TABLE data.bank`)
	sqlDB.Exec(t, `TRUNCATE TABLE data.bank`)
	sqlDB.Exec(t, `TRUNCATE TABLE data.bank`)
	sqlDB.Exec(t, `ALTER TABLE data.sometable RENAME TO other.sometable`)
	sqlDB.Exec(t, `CREATE INDEX ON data.teller (name)`)
	sqlDB.Exec(t, `INSERT INTO data.bank VALUES (2, 2), (4, 4)`)
	sqlDB.Exec(t, `INSERT INTO data.teller VALUES (2, 'craig')`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[6])

	sqlDB.Exec(t, `TRUNCATE TABLE data.bank`)
	sqlDB.Exec(t, `INSERT INTO data.bank VALUES (2, 2), (4, 4)`)
	sqlDB.Exec(t, `DROP TABLE other.sometable`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[7])

	sqlDB.Exec(t, `UPSERT INTO data.bank (id, balance)
	           SELECT i, 4 FROM generate_series(0, $1 - 1) AS g(i)`, numAccounts)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&ts[8])

	//原因在函数startDumpJob的141行，由于sometable的parentID是schema，它直接找到databaseID.
	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP DATABASE data TO SST $1 AS OF SYSTEM TIME %s INCREMENTAL FROM $2 WITH revision_history,online`, ts[5]),
		incBackup, fullBackup,
	)

	sqlDB.Exec(t,
		`DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2, $3 WITH revision_history,online`,
		inc2Backup, fullBackup, incBackup,
	)

	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP DATABASE data TO SST $1 AS OF SYSTEM TIME %s  INCREMENTAL FROM $2 with online`, ts[5]),
		incLatestBackup, latestBackup,
	)
	sqlDB.Exec(t,
		`DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2, $3 with online`,
		inc2LatestBackup, latestBackup, incLatestBackup,
	)

	incTableBackup := filepath.Join(dir, "inctbl")
	sqlDB.Exec(t,
		`DUMP data.bank TO SST $1 INCREMENTAL FROM $2 WITH revision_history,online`,
		incTableBackup, fullTableBackup,
	)

	var after string
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&after)

	for i, timestamp := range ts {
		name := fmt.Sprintf("ts%d", i)
		t.Run(name, func(t *testing.T) {
			sqlDB = sqlutils.MakeSQLRunner(sqlDB.DB)
			// Create new DBs into which we'll restore our copies without conflicting
			// with the existing, original table.
			sqlDB.Exec(t, fmt.Sprintf(`CREATE DATABASE %s`, name))
			sqlDB.Exec(t, fmt.Sprintf(`CREATE DATABASE %stbl`, name))
			// Restore the bank table from the full DB MVCC backup to time x, into a
			// separate DB so that we can later compare it to the original table via
			// time-travel.
			sqlDB.Exec(t,
				fmt.Sprintf(
					`LOAD TABLE data.bank FROM $1, $2, $3 AS OF SYSTEM TIME %s WITH into_db='%s'`,
					timestamp, name,
				),
				fullBackup, incBackup, inc2Backup,
			)
			// Similarly restore the since-table backup -- since full DB and single table
			// backups sometimes behave differently.
			sqlDB.Exec(t,
				fmt.Sprintf(
					`LOAD TABLE data.bank FROM $1, $2 AS OF SYSTEM TIME %s WITH into_db='%stbl'`,
					timestamp, name,
				),
				fullTableBackup, incTableBackup,
			)

			// Use time-travel on the existing bank table to determine what LOAD
			// with AS OF should have produced.
			expected := sqlDB.QueryStr(
				t, fmt.Sprintf(`SELECT * FROM data.bank AS OF SYSTEM TIME %s ORDER BY id`, timestamp),
			)
			// Confirm reading (with no as-of) from the as-of restored table matches.
			sqlDB.CheckQueryResults(t, fmt.Sprintf(`SELECT * FROM %s.bank ORDER BY id`, name), expected)
			sqlDB.CheckQueryResults(t, fmt.Sprintf(`SELECT * FROM %stbl.bank ORDER BY id`, name), expected)

			// `sometable` moved in to data between after ts 3 and removed before 5.
			if i == 4 || i == 5 {
				sqlDB.Exec(t,
					fmt.Sprintf(
						`LOAD TABLE data.sometable FROM $1, $2 AS OF SYSTEM TIME %s WITH into_db='%s'`,
						timestamp, name,
					),
					fullBackup, incBackup,
				)
				sqlDB.CheckQueryResults(t,
					fmt.Sprintf(`SELECT * FROM %s.sometable ORDER BY id`, name),
					sqlDB.QueryStr(t, fmt.Sprintf(`SELECT * FROM data.sometable AS OF SYSTEM TIME %s ORDER BY id`, timestamp)),
				)
			}
			// teller was created after ts 2.
			if i > 2 {
				sqlDB.Exec(t,
					fmt.Sprintf(
						`LOAD TABLE data.teller FROM $1, $2, $3 AS OF SYSTEM TIME %s WITH into_db='%s'`,
						timestamp, name,
					),
					fullBackup, incBackup, inc2Backup,
				)
				sqlDB.CheckQueryResults(t,
					fmt.Sprintf(`SELECT * FROM %s.teller ORDER BY id`, name),
					sqlDB.QueryStr(t, fmt.Sprintf(`SELECT * FROM data.teller AS OF SYSTEM TIME %s ORDER BY id`, timestamp)),
				)
			}
		})
	}

	t.Run("latest", func(t *testing.T) {
		sqlDB = sqlutils.MakeSQLRunner(sqlDB.DB)
		// The "latest" backup didn't specify ALL mvcc values, so we can't restore
		// to times in the middle.
		sqlDB.Exec(t, `CREATE DATABASE err`)

		// fullBackup covers up to ts[2], inc to ts[5], inc2 to > ts[8].
		sqlDB.ExpectErr(
			t, "incompatible LOAD timestamp",
			fmt.Sprintf(`LOAD TABLE data.bank FROM $1 AS OF SYSTEM TIME %s WITH into_db='err'`, ts[3]),
			fullBackup,
		)

		for _, i := range ts {
			sqlDB.ExpectErr(
				t, "incompatible LOAD timestamp",
				fmt.Sprintf(`LOAD TABLE data.bank FROM $1 AS OF SYSTEM TIME %s WITH into_db='err'`, i),
				latestBackup,
			)

			sqlDB.ExpectErr(
				t, "incompatible LOAD timestamp",
				fmt.Sprintf(`LOAD TABLE data.bank FROM $1, $2, $3 AS OF SYSTEM TIME %s WITH into_db='err'`, i),
				latestBackup, incLatestBackup, inc2LatestBackup,
			)
		}

		sqlDB.ExpectErr(
			t, "incompatible LOAD timestamp",
			fmt.Sprintf(`LOAD TABLE data.bank FROM $1 AS OF SYSTEM TIME %s WITH into_db='err'`, after),
			latestBackup,
		)
	})
}

func TestLoadAsOfSystemTimeGCBounds(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 10
	ctx, tc, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	const dir = "nodelocal:///"
	preGC := tree.TimestampToDecimal(tc.Server(0).Clock().Now()).String()

	gcr := roachpb.GCRequest{
		// Bogus span to make it a valid request.
		RequestHeader: roachpb.RequestHeader{
			Key:    keys.MakeTablePrefix(keys.MinUserDescID),
			EndKey: keys.MaxKey,
		},
		Threshold: tc.Server(0).Clock().Now(),
	}
	if _, err := client.SendWrapped(ctx, tc.Server(0).DistSender().(*kv.DistSender), &gcr); err != nil {
		t.Fatal(err)
	}

	postGC := tree.TimestampToDecimal(tc.Server(0).Clock().Now()).String()

	lateFullTableBackup := filepath.Join(dir, "tbl-after-gc")
	sqlDB.Exec(t, `DUMP data.bank TO SST $1 WITH revision_history`, lateFullTableBackup)
	sqlDB.Exec(t, `DROP TABLE data.bank`)
	sqlDB.ExpectErr(
		t, `DUMP for requested time only has revision history from`,
		fmt.Sprintf(`LOAD TABLE data.bank FROM $1 AS OF SYSTEM TIME %s`, preGC),
		lateFullTableBackup,
	)
	sqlDB.Exec(
		t, fmt.Sprintf(`LOAD TABLE data.bank FROM $1 AS OF SYSTEM TIME %s`, postGC), lateFullTableBackup,
	)
}

func TestAsOfSystemTimeOnLoadData(t *testing.T) {
	defer leaktest.AfterTest(t)()

	_, _, sqlDB, dir, cleanupFn := dumpLoadTestSetup(t, singleNode, 0, initNone)
	defer cleanupFn()

	sqlDB.Exec(t, `DROP TABLE data.bank`)

	// 增加部分延迟已避免HLC引发的问题
	var beforeTs string
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&beforeTs)

	const numAccounts = 10
	bankData := bank.FromRows(numAccounts).Tables()[0]
	if _, err := sampledataicl.ToBackup(t, bankData, filepath.Join(dir, "foo")); err != nil {
		t.Fatalf("%+v", err)
	}

	sqlDB.Exec(t, `LOAD TABLE data.bank FROM $1`, localFoo)
	var afterTs string
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&afterTs)

	var rowCount int
	const q = `SELECT count(*) FROM data.bank AS OF SYSTEM TIME '%s'`
	// Before the LOAD, the table doesn't exist, so an AS OF query should fail.
	sqlDB.ExpectErr(
		t, `relation "data.bank" does not exist`,
		fmt.Sprintf(q, beforeTs),
	)
	// After the LOAD, an AS OF query should work.
	sqlDB.QueryRow(t, fmt.Sprintf(q, afterTs)).Scan(&rowCount)
	if expected := numAccounts; rowCount != expected {
		t.Fatalf("expected %d rows but found %d", expected, rowCount)
	}
}

func TestDumpLoadChecksum(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1000
	_, _, sqlDB, dir, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	dir = filepath.Join(dir, "foo")

	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1`, localFoo)

	var backupDesc dump.DumpDescriptor
	{
		backupDescBytes, err := ioutil.ReadFile(filepath.Join(dir, dump.DumpDescriptorName))
		if err != nil {
			t.Fatalf("%+v", err)
		}
		if err := protoutil.Unmarshal(backupDescBytes, &backupDesc); err != nil {
			t.Fatalf("%+v", err)
		}
	}

	// Corrupt one of the files in the backup.
	f, err := os.OpenFile(filepath.Join(dir, backupDesc.Files[1].Path), os.O_WRONLY, 0)
	if err != nil {
		t.Fatalf("%+v", err)
	}
	defer f.Close()
	// The last eight bytes of an SST file store a nonzero magic number. We can
	// blindly null out those bytes and guarantee that the checksum will change.
	if _, err := f.Seek(-8, io.SeekEnd); err != nil {
		t.Fatalf("%+v", err)
	}
	if _, err := f.Write(make([]byte, 8)); err != nil {
		t.Fatalf("%+v", err)
	}
	if err := f.Sync(); err != nil {
		t.Fatalf("%+v", err)
	}

	sqlDB.Exec(t, `DROP TABLE data.bank`)
	sqlDB.ExpectErr(t, "checksum mismatch", `LOAD TABLE data.bank FROM $1`, localFoo)
}

func TestTimestampMismatch(t *testing.T) {
	defer leaktest.AfterTest(t)()
	const numAccounts = 1

	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	sqlDB.Exec(t, `CREATE TABLE data.t2 (a INT PRIMARY KEY)`)
	sqlDB.Exec(t, `INSERT INTO data.t2 VALUES (1)`)

	fullBackup := filepath.Join(localFoo, "0")
	incrementalT1FromFull := filepath.Join(localFoo, "1")
	incrementalT2FromT1 := filepath.Join(localFoo, "2")
	incrementalT3FromT1OneTable := filepath.Join(localFoo, "3")

	dumpDir1 := filepath.Join(localFoo, "dump1")
	dumpDir2 := filepath.Join(localFoo, "dump2")
	dumpDir3 := filepath.Join(localFoo, "dump3")

	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1`,
		fullBackup)
	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2`,
		incrementalT1FromFull, fullBackup)
	sqlDB.Exec(t, `DUMP TABLE data.bank TO SST $1 INCREMENTAL FROM $2`,
		incrementalT3FromT1OneTable, fullBackup)
	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2, $3`,
		incrementalT2FromT1, fullBackup, incrementalT1FromFull)

	t.Run("Dump", func(t *testing.T) {
		// Missing the initial full backup.
		sqlDB.ExpectErr(
			t, "dump listed out of order",
			`DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2`,
			localFoo, incrementalT1FromFull,
		)

		// Missing an intermediate incremental backup.
		sqlDB.ExpectErr(
			t, "dump listed out of order",
			`DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2, $3`,
			dumpDir1, fullBackup, incrementalT2FromT1,
		)

		// Backups specified out of order.
		sqlDB.ExpectErr(
			t, "out of order",
			`DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2, $3`,
			dumpDir2, incrementalT1FromFull, fullBackup,
		)

		// Missing data for one table in the most recent backup.
		sqlDB.ExpectErr(
			t, "previous dump does not contain table",
			`DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2, $3`,
			dumpDir3, fullBackup, incrementalT3FromT1OneTable,
		)
	})

	sqlDB.Exec(t, `DROP TABLE data.bank`)
	sqlDB.Exec(t, `DROP TABLE data.t2`)
	t.Run("Restore", func(t *testing.T) {
		// Missing the initial full backup.
		sqlDB.ExpectErr(t, "dump listed out of order", `LOAD TABLE data.bank FROM $1`, incrementalT1FromFull)

		// Missing an intermediate incremental backup.
		sqlDB.ExpectErr(
			//官方版是还原data.*所有表，必然会没有covers time。
			t, "no dump covers time",
			`LOAD TABLE data.bank FROM $1, $2`, fullBackup, incrementalT2FromT1,
		)

		// Backups specified out of order.
		sqlDB.ExpectErr(
			t, "out of order",
			`LOAD TABLE data.bank FROM $1, $2`, incrementalT1FromFull, fullBackup,
		)

		//Missing data for one table in the most recent backup.
		sqlDB.ExpectErr(
			t, "table \"data.t2\" does not exist",
			`LOAD TABLE data.t2 FROM $1, $2`, fullBackup, incrementalT3FromT1OneTable,
		)
	})
}

func TestDumpPebbleDB(t *testing.T) {
	defer leaktest.AfterTest(t)()

	_, _, sqlDB, rawDir, cleanupFn := dumpLoadTestSetup(t, singleNode, 1, initNone)
	defer cleanupFn()

	_ = sqlDB.Exec(t, `DUMP DATABASE data TO SST $1`, localFoo)
	// Verify that the sstables are in LevelDB format by checking the trailer
	// magic.
	//var leveldbMagic = []byte("\x57\xfb\x80\x8b\x24\x75\x47\xdb")
	var pebbleDBmagic = []byte("\xf7\xcf\xf4\x85\xb7\x41\xe2\x88")
	foundSSTs := 0
	if err := filepath.Walk(rawDir, func(path string, info os.FileInfo, err error) error {
		if filepath.Ext(path) == ".sst" {
			foundSSTs++
			data, err := ioutil.ReadFile(path)
			if err != nil {
				t.Fatal(err)
			}
			if !bytes.HasSuffix(data, pebbleDBmagic) {
				t.Fatalf("trailer magic is not LevelDB sstable: %s", path)
			}
		}
		return nil
	}); err != nil {
		t.Fatalf("%+v", err)
	}
	if foundSSTs == 0 {
		t.Fatal("found no sstables")
	}
}

func TestLoadPrivileges(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, dir, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	args := base.TestServerArgs{ExternalIODir: dir}

	sqlDB.Exec(t, `CREATE USER someone`)
	sqlDB.Exec(t, `GRANT SELECT, INSERT, UPDATE, DELETE ON data.bank TO someone`)

	withGrants := sqlDB.QueryStr(t, `SHOW GRANTS ON data.bank`)

	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1`, localFoo)
	sqlDB.Exec(t, `DROP TABLE data.bank`)

	t.Run("into fresh db", func(t *testing.T) {
		tc := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tc.Stopper().Stop(context.TODO())
		sqlDBRestore := sqlutils.MakeSQLRunner(tc.Conns[0])
		sqlDBRestore.Exec(t, `CREATE DATABASE data`)
		sqlDBRestore.Exec(t, `LOAD TABLE data.bank FROM $1`, localFoo)
		sqlDBRestore.CheckQueryResults(t, `SHOW GRANTS ON data.bank`, withGrants)
	})

	t.Run("into db with added grants", func(t *testing.T) {
		tc := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tc.Stopper().Stop(context.TODO())
		sqlDBRestore := sqlutils.MakeSQLRunner(tc.Conns[0])
		sqlDBRestore.Exec(t, `CREATE DATABASE data`)
		sqlDBRestore.Exec(t, `CREATE USER someone`)
		sqlDBRestore.Exec(t, `GRANT ALL ON DATABASE data TO someone`)
		sqlDBRestore.Exec(t, `LOAD TABLE data.bank FROM $1`, localFoo)
		sqlDBRestore.CheckQueryResults(t, `SHOW GRANTS ON data.bank`, withGrants)
	})
}

func TestLoadInto(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1`, localFoo)

	restoreStmt := fmt.Sprintf(`LOAD TABLE data.bank FROM '%s' WITH into_db = 'data 2'`, localFoo)

	sqlDB.ExpectErr(t, "a database named \"data 2\" needs to exist", restoreStmt)

	sqlDB.Exec(t, `CREATE DATABASE "data 2"`)
	sqlDB.Exec(t, restoreStmt)

	expected := sqlDB.QueryStr(t, `SELECT * FROM data.bank`)
	sqlDB.CheckQueryResults(t, `SELECT * FROM "data 2".bank`, expected)
}

func TestDumpLoadPermissions(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, tc, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	sqlDB.Exec(t, `CREATE USER testuser`)
	sqlDB.Exec(t, `GRANT USAGE ON DATABASE data TO testuser`)
	pgURL, cleanupFunc := sqlutils.PGUrl(
		t, tc.Server(0).ServingAddr(), "TestDumpLoadPermissions-testuser", url.User("testuser"),
	)
	defer cleanupFunc()
	testuser, err := gosql.Open("postgres", pgURL.String())
	if err != nil {
		t.Fatal(err)
	}
	defer testuser.Close()

	backupStmt := fmt.Sprintf(`DUMP DATABASE data TO SST '%s'`, localFoo)

	t.Run("root-only", func(t *testing.T) {
		if _, err := testuser.Exec(backupStmt); !testutils.IsError(
			err, "pq: user testuser does not have SELECT privilege on relation data.public.bank",
		) {
			t.Fatal(err)
		}

		// 'blah' 是一个类似nodelocal:///目录, blah是一个事务，官方原版的TestBackupRestoreNotInTxn给出了
		// DUMP blah to sst 'blah'和LOAD blah FROM 'blah',这里的文件没有TestBackupRestoreNotInTxn测试。
		// 暂时把LOAD blah FROM 'blah'去掉
		//if _, err := testuser.Exec(`LOAD blah FROM 'blah'`); !testutils.IsError(
		//	err, "only superusers are allowed to LOAD",
		//) {
		//	t.Fatal(err)
		//}
	})

	t.Run("privs-required", func(t *testing.T) {
		sqlDB.Exec(t, backupStmt)
		// Root doesn't have CREATE on `system` DB, so that should fail. Still need
		// a valid `dir` though, since descriptors are always loaded first.
		sqlDB.ExpectErr(
			t, "user root does not have CREATE privilege",
			`LOAD TABLE data.bank FROM $1 WITH OPTIONS ('into_db'='system')`, localFoo,
		)
	})
}

func TestLoadDatabaseVersusTable(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, tc, origDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	args := base.TestServerArgs{ExternalIODir: tc.Servers[0].ClusterSettings().ExternalIODir}

	for _, q := range []string{
		`CREATE DATABASE d2`,
		`CREATE DATABASE d3`,
		`CREATE TABLE d3.foo (a INT)`,
		`CREATE DATABASE d4`,
		`CREATE TABLE d4.foo (a INT)`,
		`CREATE TABLE d4.bar (a INT)`,
	} {
		origDB.Exec(t, q)
	}

	d4foo := "nodelocal:///d4foo"
	d4foobar := "nodelocal:///d4foobar"
	d4star := "nodelocal:///d4star"

	origDB.Exec(t, `DUMP DATABASE data, d2, d3, d4 TO SST $1`, localFoo)
	origDB.Exec(t, `DUMP d4.foo TO SST $1`, d4foo)
	origDB.Exec(t, `DUMP d4.foo, d4.bar TO SST $1`, d4foobar)
	origDB.Exec(t, `DUMP d4.* TO SST $1`, d4star)

	t.Run("incomplete-db", func(t *testing.T) {
		tcRestore := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tcRestore.Stopper().Stop(context.TODO())
		sqlDB := sqlutils.MakeSQLRunner(tcRestore.Conns[0])

		sqlDB.Exec(t, `create database d5`)

		sqlDB.ExpectErr(
			t, "cannot LOAD DATABASE from a backup of individual tables",
			`LOAD DATABASE d4 FROM $1`, d4foo,
		)

		sqlDB.ExpectErr(
			t, "table \"d4.bar\" does not exist",
			`LOAD TABLE d4.bar FROM $1 WITH into_db = 'd5'`, d4star,
		)

		sqlDB.ExpectErr(
			t, "cannot LOAD DATABASE from a backup of individual tables",
			`LOAD database d4 FROM $1`, d4foobar,
		)

		sqlDB.ExpectErr(
			t, "",
			`LOAD TABLE d4.bar FROM $1 WITH into_db = 'd5'`, d4foobar,
		)

		sqlDB.ExpectErr(
			t, "cannot LOAD DATABASE from a backup of individual tables",
			`LOAD DATABASE d4 FROM $1`, d4foo,
		)
		//目前dump d4.*，和load database不对接，load database只对接dump database
		sqlDB.ExpectErr(t, "unknown database \"d4\"",
			`LOAD DATABASE d4 FROM $1`, d4star,
		)
	})

	t.Run("db", func(t *testing.T) {
		tcRestore := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tcRestore.Stopper().Stop(context.TODO())
		sqlDB := sqlutils.MakeSQLRunner(tcRestore.Conns[0])
		sqlDB.Exec(t, `LOAD DATABASE data FROM $1`, localFoo)
	})

	t.Run("db-exists", func(t *testing.T) {
		tcRestore := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tcRestore.Stopper().Stop(context.TODO())
		sqlDB := sqlutils.MakeSQLRunner(tcRestore.Conns[0])

		sqlDB.Exec(t, `CREATE DATABASE data`)
		sqlDB.ExpectErr(t, "already exists", `LOAD DATABASE data FROM $1`, localFoo)
	})

	t.Run("tables", func(t *testing.T) {
		tcRestore := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tcRestore.Stopper().Stop(context.TODO())
		sqlDB := sqlutils.MakeSQLRunner(tcRestore.Conns[0])

		//sqlDB.Exec(t, `CREATE DATABASE data`)
		sqlDB.Exec(t, `LOAD DATABASE data FROM $1`, localFoo)
	})

	t.Run("tables-needs-db", func(t *testing.T) {
		tcRestore := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tcRestore.Stopper().Stop(context.TODO())
		sqlDB := sqlutils.MakeSQLRunner(tcRestore.Conns[0])

		//还原整个库的所有表没有问题，和table无关，原来是 load d4.*等价于还原库database所有表
		sqlDB.ExpectErr(t, "", `LOAD DATABASE d4 FROM $1`, localFoo)
	})

	t.Run("into_db", func(t *testing.T) {
		tcRestore := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tcRestore.Stopper().Stop(context.TODO())
		sqlDB := sqlutils.MakeSQLRunner(tcRestore.Conns[0])

		sqlDB.ExpectErr(
			t, `cannot use "into_db"`,
			`LOAD DATABASE data FROM $1 WITH into_db = 'other'`, localFoo,
		)
	})
}

func TestDumpAzureAccountName(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	values := url.Values{}
	values.Set("AZURE_ACCOUNT_KEY", "password")
	values.Set("AZURE_ACCOUNT_NAME", "\n")

	url := &url.URL{
		Scheme:   "azure",
		Host:     "host",
		Path:     "/backup",
		RawQuery: values.Encode(),
	}

	// Verify newlines in the account name cause an error.
	// dump暂时不支持AZURE功能
	//sqlDB.ExpectErr(t, "azure: account name is not valid", `dump database d to sst $1`, url.String())
	sqlDB.ExpectErr(t, "unsupported storage scheme: \"azure\"", `dump database d to sst $1`, url.String())
}

// If an operator issues a bad query or if a deploy contains a bug that corrupts
// data, it should be possible to return to a previous point in time before the
// badness. For cases when the last good timestamp is within the gc threshold,
// see the subtests for two ways this can work.
func TestPointInTimeRecovery(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1000
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	fullBackupDir := filepath.Join(localFoo, "full")
	sqlDB.Exec(t, `DUMP data.* TO SST $1`, fullBackupDir)

	sqlDB.Exec(t, `UPDATE data.bank SET balance = 2`)

	incBackupDir := filepath.Join(localFoo, "inc")
	sqlDB.Exec(t, `DUMP data.* TO SST $1 INCREMENTAL FROM $2`, incBackupDir, fullBackupDir)

	var beforeBadThingTs string
	sqlDB.Exec(t, `UPDATE data.bank SET balance = 3`)
	sqlDB.QueryRow(t, `SELECT cluster_logical_timestamp()`).Scan(&beforeBadThingTs)

	// Something bad happens.
	sqlDB.Exec(t, `UPDATE data.bank SET balance = 4`)

	beforeBadThingData := sqlDB.QueryStr(t,
		fmt.Sprintf(`SELECT * FROM data.bank AS OF SYSTEM TIME '%s' ORDER BY id`, beforeBadThingTs),
	)

	// If no previous DUMP have been taken, a new one can be taken using `AS
	// OF SYSTEM TIME` with a timestamp before the badness started. This can
	// then be LOAD'd into a temporary database. The operator can manually
	// reconcile the current data with the restored data before finally
	// RENAME-ing the table into the final location.
	t.Run("recovery=new-backup", func(t *testing.T) {
		sqlDB = sqlutils.MakeSQLRunner(sqlDB.DB)
		recoveryDir := filepath.Join(localFoo, "new-backup")
		sqlDB.Exec(t,
			fmt.Sprintf(`DUMP data.* TO SST $1 AS OF SYSTEM TIME '%s'`, beforeBadThingTs),
			recoveryDir,
		)
		sqlDB.Exec(t, `CREATE DATABASE newbackup`)
		//还原使用into_db时，只能针对表，不能还原database
		sqlDB.Exec(t, `LOAD TABLE data.bank FROM $1 WITH into_db=newbackup`, recoveryDir)

		// Some manual reconciliation of the data in data.bank and
		// newbackup.bank could be done here by the operator.

		sqlDB.Exec(t, `DROP TABLE data.bank`)
		sqlDB.Exec(t, `ALTER TABLE newbackup.bank RENAME TO data.bank`)
		sqlDB.Exec(t, `DROP DATABASE newbackup`)
		sqlDB.CheckQueryResults(t, `SELECT * FROM data.bank ORDER BY id`, beforeBadThingData)
	})

	// If there is a recent DUMP (either full or incremental), then it will
	// likely be faster to make a DUMP that is incremental from it and LOAD
	// using that. Everything else works the same as above.
	t.Run("recovery=inc-backup", func(t *testing.T) {
		sqlDB = sqlutils.MakeSQLRunner(sqlDB.DB)
		recoveryDir := filepath.Join(localFoo, "inc-backup")
		sqlDB.Exec(t,
			fmt.Sprintf(`DUMP data.* TO SST $1 AS OF SYSTEM TIME '%s' INCREMENTAL FROM $2, $3`, beforeBadThingTs),
			recoveryDir, fullBackupDir, incBackupDir,
		)
		sqlDB.Exec(t, `CREATE DATABASE incbackup`)
		sqlDB.Exec(t,
			//还原使用into_db时，只能针对表，不能还原database
			`LOAD TABLE data.bank FROM $1, $2, $3 WITH into_db=incbackup`,
			fullBackupDir, incBackupDir, recoveryDir,
		)

		// Some manual reconciliation of the data in data.bank and
		// incbackup.bank could be done here by the operator.

		sqlDB.Exec(t, `DROP TABLE data.bank`)
		sqlDB.Exec(t, `ALTER TABLE incbackup.bank RENAME TO data.bank`)
		sqlDB.Exec(t, `DROP DATABASE incbackup`)
		sqlDB.CheckQueryResults(t, `SELECT * FROM data.bank ORDER BY id`, beforeBadThingData)
	})
}

func TestDumpLoadDropDB(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	sqlDB.Exec(t, `DROP DATABASE data`)
	sqlDB.Exec(t, `CREATE DATABASE data`)
	sqlDB.Exec(t, `CREATE TABLE data.bank (i int)`)
	sqlDB.Exec(t, `INSERT INTO data.bank VALUES (1)`)

	sqlDB.Exec(t, "DUMP DATABASE data TO SST $1", localFoo)
	sqlDB.Exec(t, "CREATE DATABASE data2")
	sqlDB.Exec(t, "LOAD TABLE data.bank FROM $1 WITH OPTIONS ('into_db'='data2')", localFoo)

	expected := sqlDB.QueryStr(t, `SELECT * FROM data.bank`)
	sqlDB.CheckQueryResults(t, `SELECT * FROM data2.bank`, expected)
}

func TestDumpLoadDropTable(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	sqlDB.Exec(t, `DROP TABLE data.bank`)
	sqlDB.Exec(t, `
		CREATE TABLE data.bank (i int);
		INSERT INTO data.bank VALUES (1);
	`)

	sqlDB.Exec(t, "DUMP DATABASE data TO SST $1", localFoo)
	sqlDB.Exec(t, "CREATE DATABASE data2")
	sqlDB.Exec(t, "LOAD TABLE data.bank FROM $1 WITH OPTIONS ('into_db'='data2')", localFoo)

	expected := sqlDB.QueryStr(t, `SELECT * FROM data.bank`)
	sqlDB.CheckQueryResults(t, `SELECT * FROM data2.bank`, expected)
}

func TestDumpLoadIncrementalAddTable(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	sqlDB.Exec(t, `CREATE DATABASE data2`)
	sqlDB.Exec(t, `CREATE TABLE data.t (s string PRIMARY KEY)`)
	full, inc := filepath.Join(localFoo, "full"), filepath.Join(localFoo, "inc")

	sqlDB.Exec(t, `INSERT INTO data.t VALUES ('before')`)
	sqlDB.Exec(t, `DUMP data.*, data2.* TO SST $1`, full)
	sqlDB.Exec(t, `UPDATE data.t SET s = 'after'`)

	sqlDB.Exec(t, `CREATE TABLE data2.t2 (i int)`)
	sqlDB.Exec(t, "DUMP data.*, data2.* TO SST $1 INCREMENTAL FROM $2", inc, full)
}

func TestDumpLoadIncrementalAddTableMissing(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	sqlDB.Exec(t, `CREATE DATABASE data2`)
	sqlDB.Exec(t, `CREATE TABLE data.t (s string PRIMARY KEY)`)
	full, inc := filepath.Join(localFoo, "full"), filepath.Join(localFoo, "inc")

	sqlDB.Exec(t, `INSERT INTO data.t VALUES ('before')`)
	sqlDB.Exec(t, `DUMP data.* TO SST $1`, full)
	sqlDB.Exec(t, `UPDATE data.t SET s = 'after'`)

	sqlDB.Exec(t, `CREATE TABLE data2.t2 (i int)`)
	sqlDB.ExpectErr(
		//属于正常备份情况
		t, "",
		"DUMP data.*, data2.* TO SST $1 INCREMENTAL FROM $2", inc, full,
	)
}

func TestDumpLoadIncrementalTrucateTable(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	sqlDB.Exec(t, `CREATE TABLE data.t (s string PRIMARY KEY)`)
	full, inc := filepath.Join(localFoo, "full"), filepath.Join(localFoo, "inc")

	sqlDB.Exec(t, `INSERT INTO data.t VALUES ('before')`)
	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1`, full)
	sqlDB.Exec(t, `UPDATE data.t SET s = 'after'`)
	sqlDB.Exec(t, `TRUNCATE data.t`)

	sqlDB.Exec(t, "DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2", inc, full)
}

func TestDumpLoadIncrementalDropTable(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	sqlDB.Exec(t, `CREATE TABLE data.t (s string PRIMARY KEY)`)
	full, inc := filepath.Join(localFoo, "full"), filepath.Join(localFoo, "inc")

	sqlDB.Exec(t, `INSERT INTO data.t VALUES ('before')`)
	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1`, full)
	sqlDB.Exec(t, `UPDATE data.t SET s = 'after'`)
	sqlDB.Exec(t, `DROP TABLE data.t`)

	sqlDB.Exec(t, "DUMP DATABASE data TO SST $1 INCREMENTAL FROM $2", inc, full)
	sqlDB.Exec(t, `DROP DATABASE data`)

	// Restoring to backup before DROP restores t.
	sqlDB.Exec(t, `LOAD DATABASE data FROM $1`, full)
	sqlDB.Exec(t, `SELECT 1 FROM data.t LIMIT 0`)
	sqlDB.Exec(t, `DROP DATABASE data`)

	// Restoring to backup after DROP does not restore t.
	sqlDB.Exec(t, `LOAD DATABASE data FROM $1, $2`, full, inc)
	sqlDB.ExpectErr(t, "relation \"data.t\" does not exist", `SELECT 1 FROM data.t LIMIT 0`)
}

func TestFileIOLimits(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 11
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	elsewhere := "nodelocal:///../../blah"

	sqlDB.Exec(t, `DUMP data.bank TO SST $1`, localFoo)
	sqlDB.ExpectErr(
		t, "local file access to paths outside of external-io-dir is not allowed",
		`DUMP data.bank TO SST $1`, elsewhere,
	)

	sqlDB.Exec(t, `DROP TABLE data.bank`)

	sqlDB.Exec(t, `LOAD TABLE data.bank FROM $1`, localFoo)
	sqlDB.ExpectErr(
		t, "local file access to paths outside of external-io-dir is not allowed",
		`LOAD TABLE data.bank FROM $1`, elsewhere,
	)
}

func TestDumpLoadNotInTxn(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	db := sqlDB.DB.(*gosql.DB)
	tx, err := db.Begin()
	if err != nil {
		t.Fatal(err)
	}
	if _, err := tx.Exec(`DUMP DATABASE data TO SST 'blah'`); !testutils.IsError(err, "cannot be used inside a transaction") {
		t.Fatal(err)
	}
	if err := tx.Rollback(); err != nil {
		t.Fatal(err)
	}

	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1`, localFoo)
	sqlDB.Exec(t, `DROP DATABASE data`)
	sqlDB.Exec(t, `LOAD DATABASE data FROM $1`, localFoo)

	tx, err = db.Begin()
	if err != nil {
		t.Fatal(err)
	}
	if _, err := tx.Exec(`LOAD DATABASE data FROM 'blah'`); !testutils.IsError(err, "cannot be used inside a transaction") {
		t.Fatal(err)
	}
	if err := tx.Rollback(); err != nil {
		t.Fatal(err)
	}

	// TODO(dt): move to importccl.
	tx, err = db.Begin()
	if err != nil {
		t.Fatal(err)
	}
	if _, err := tx.Exec(`LOAD TABLE t(id INT PRIMARY KEY) CSV DATA ('blah')`); !testutils.IsError(err, "cannot be used inside a transaction") {
		t.Fatal(err)
	}
	if err := tx.Rollback(); err != nil {
		t.Fatal(err)
	}
}

func TestDumpLoadSequence(t *testing.T) {
	defer leaktest.AfterTest(t)()
	const numAccounts = 1
	_, _, origDB, dir, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	args := base.TestServerArgs{ExternalIODir: dir}

	backupLoc := localFoo

	origDB.Exec(t, `CREATE SEQUENCE data.t_id_seq`)
	origDB.Exec(t, `CREATE TABLE data.t (id INT PRIMARY KEY DEFAULT nextval('data.t_id_seq'), v text)`)
	origDB.Exec(t, `INSERT INTO data.t (v) VALUES ('foo'), ('bar'), ('baz')`)

	origDB.Exec(t, `DUMP DATABASE data TO SST $1`, backupLoc)

	t.Run("restore both table & sequence to a new cluster", func(t *testing.T) {
		tc := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tc.Stopper().Stop(context.TODO())
		newDB := sqlutils.MakeSQLRunner(tc.Conns[0])

		newDB.Exec(t, `LOAD DATABASE data FROM $1`, backupLoc)
		newDB.Exec(t, `USE data`)

		// Verify that the db was restored correctly.
		newDB.CheckQueryResults(t, `SELECT * FROM t`, [][]string{
			{"1", "foo"},
			{"2", "bar"},
			{"3", "baz"},
		})
		newDB.CheckQueryResults(t, `SELECT last_value FROM t_id_seq`, [][]string{
			{"3"},
		})

		// Verify that we can kkeep inserting into the table, without violating a uniqueness constraint.
		newDB.Exec(t, `INSERT INTO data.t (v) VALUES ('bar')`)

		// Verify that sequence <=> table dependencies are still in place.
		newDB.ExpectErr(
			t, "pq: cannot drop sequence t_id_seq because other objects depend on it",
			`DROP SEQUENCE t_id_seq`,
		)
	})

	t.Run("load just the table to a new cluster", func(t *testing.T) {
		tc := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tc.Stopper().Stop(context.TODO())
		newDB := sqlutils.MakeSQLRunner(tc.Conns[0])

		newDB.Exec(t, `CREATE DATABASE data`)
		newDB.Exec(t, `USE data`)

		newDB.ExpectErr(
			t, "pq: cannot restore table \"t\" without referenced sequence 57 \\(or \"skip_missing_sequences\" option\\)",
			`LOAD TABLE t FROM $1`, localFoo,
		)

		newDB.Exec(t, `LOAD TABLE t FROM $1 WITH skip_missing_sequences`, localFoo)

		// Verify that the table was restored correctly.
		newDB.CheckQueryResults(t, `SELECT * FROM data.t`, [][]string{
			{"1", "foo"},
			{"2", "bar"},
			{"3", "baz"},
		})

		// Test that insertion without specifying the id column doesn't work, since
		// the DEFAULT expression has been removed.
		newDB.ExpectErr(
			t, `pq: missing \"id\" primary key column`,
			`INSERT INTO t (v) VALUES ('bloop')`,
		)

		// Test that inserting with a value specified works.
		newDB.Exec(t, `INSERT INTO t (id, v) VALUES (4, 'bloop')`)
	})

	t.Run("restore just the sequence to a new cluster", func(t *testing.T) {
		tc := testcluster.StartTestCluster(t, singleNode, base.TestClusterArgs{ServerArgs: args})
		defer tc.Stopper().Stop(context.TODO())
		newDB := sqlutils.MakeSQLRunner(tc.Conns[0])

		newDB.Exec(t, `CREATE DATABASE data`)
		newDB.Exec(t, `USE data`)
		// TODO(vilterp): create `LOAD SEQUENCE` instead of `LOAD TABLE`, and force
		// people to use that?
		newDB.Exec(t, `LOAD TABLE t_id_seq FROM $1`, backupLoc)

		// Verify that the sequence value was restored.
		newDB.CheckQueryResults(t, `SELECT last_value FROM data.t_id_seq`, [][]string{
			{"3"},
		})

		// Verify that the reference to the table that used it was removed, and
		// it can be dropped.
		newDB.Exec(t, `DROP SEQUENCE t_id_seq`)
	})
}

func TestDumpLoadShowJob(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1 WITH revision_history`, localFoo)
	sqlDB.Exec(t, `CREATE DATABASE "data 2"`)

	sqlDB.Exec(t, `LOAD TABLE data.bank FROM $1 WITH skip_missing_foreign_keys, into_db = $2`, localFoo, "data 2")
	sqlDB.CheckQueryResults(t, "SELECT description FROM [SHOW JOBS] ORDER BY description", [][]string{
		{"DUMP DATABASE data TO SST 'nodelocal:///foo' WITH revision_history"},
		{"LOAD TABLE data.public.bank FROM 'nodelocal:///foo' WITH into_db = 'data 2', skip_missing_foreign_keys"},
	})
}

func TestCreateStatsAfterLoad(t *testing.T) {
	defer leaktest.AfterTest(t)()
	//TODO(jianglei02) 当租约过期的时候，拿到的东西过期之后会导致测试出问题,小概率无法通过. skip() temporarily
	t.Skip()

	defer func(oldRefreshInterval, oldAsOf time.Duration) {
		stats.DefaultRefreshInterval = oldRefreshInterval
		stats.DefaultAsOfTime = oldAsOf
	}(stats.DefaultRefreshInterval, stats.DefaultAsOfTime)
	stats.DefaultRefreshInterval = time.Millisecond
	stats.DefaultAsOfTime = time.Microsecond

	const numAccounts = 1
	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()

	sqlDB.Exec(t, `SET CLUSTER SETTING sql.stats.automatic_collection.enabled=true`)

	sqlDB.Exec(t, `DUMP DATABASE data TO SST $1 WITH revision_history`, localFoo)
	sqlDB.Exec(t, `CREATE DATABASE "data 2"`)
	sqlDB.Exec(t, `LOAD TABLE data.bank FROM $1 WITH skip_missing_foreign_keys, into_db = $2`,
		localFoo, "data 2")

	// Verify that statistics have been created.
	sqlDB.CheckQueryResultsRetry(t,
		`SELECT statistics_name, column_names, row_count, distinct_count, null_count
	  FROM [SHOW STATISTICS FOR TABLE "data 2".bank]`,
		[][]string{
			{"__auto__", "{id}", "1", "1", "0"},
			{"__auto__", "{balance}", "1", "1", "0"},
			{"__auto__", "{payload}", "1", "1", "0"},
		})
}

//用全量+增量+WAL进行恢复表
func TestRestoreWalTable(t *testing.T) {
	defer leaktest.AfterTest(t)()
	t.Skip("需要人工验证")
	s, db, _ := serverutils.StartServer(t, base.TestServerArgs{ExternalIODir: "/data/rocksdb/extern"})
	defer s.Stopper().Stop(context.TODO())
	fullDump := "nodelocal://01/foo"
	incDump := "nodelocal://01/inc1"
	walDir := "nodelocal://01/wal"

	if _, err := db.Exec(`create database data`); err != nil {
		t.Fatalf("unexpected error: %s", err)
	}
	//导入sst文件
	if _, err := db.Exec(`LOAD table data.test3 FROM $1,$2 WITH last_log_collection = $3 `, fullDump, incDump, walDir); err != nil {
		t.Fatalf("unexpected error: %s", err)
	}

	//TODO:结果检查
	rows, err := db.Query("select d from data.test3")
	if err != nil {
		t.Fatalf("unexpected error: %s", err)
	}
	if rows != nil {
		for rows.Next() {
			var table string
			_ = rows.Scan(&table)
			fmt.Printf("select results: %v\n", table)
		}
	} else {
		t.Fatalf("no results")
	}

}

//用全量+增量+WAL进行恢复库
func TestRestoreWalDatabase(t *testing.T) {
	defer leaktest.AfterTest(t)()
	t.Skip("需要人工验证")
	s, db, _ := serverutils.StartServer(t, base.TestServerArgs{ExternalIODir: "/data/wal_revert/data"})
	defer s.Stopper().Stop(context.TODO())
	fullDump := "nodelocal:///foo"
	incDump := "nodelocal:///inc"
	walDir := "nodelocal:///wal"

	//导入sst文件
	if _, err := db.Exec(`LOAD database tpcc FROM $1,$2 WITH last_log_collection = $3 `, fullDump, incDump, walDir); err != nil {
		t.Fatalf("unexpected error: %s", err)
	}

	//TODO:结果检查
	rows, err := db.Query("select count(1) from tpcc.item")
	if err != nil {
		t.Fatalf("unexpected error: %s", err)
	}
	if rows != nil {
		for rows.Next() {
			var table int
			_ = rows.Scan(&table)
			fmt.Printf("select results: %v\n", table)
		}
	} else {
		t.Fatalf("no results")
	}

}

func TestLoadView(t *testing.T) {
	defer leaktest.AfterTest(t)()
	dir, cleanupDir := testutils.TempDir(t)
	defer cleanupDir()

	srv, db, _ := serverutils.StartServer(t, base.TestServerArgs{ExternalIODir: dir})
	defer srv.Stopper().Stop(context.Background())
	sqlDB := sqlutils.MakeSQLRunner(db)

	sqlDB.Exec(t, `create table foo (i int primary key, x int, y int, z int)`)
	sqlDB.Exec(t, `insert into foo values (1, 12, 4, 14), (2, 22, 3, 24), (3, 32, 2, 34)`)
	sqlDB.Exec(t, `CREATE VIEW foov ( m1, m2, m3, m4 ) AS SELECT i,x,y,z FROM foo;`)
	sqlDB.Exec(t, `DUMP VIEW foov TO SST "nodelocal:///shows";`)
	sqlDB.Exec(t, `drop view foov`)
	sqlDB.Exec(t, `insert into foo values (5, 12, 4, 14)`)
	sqlDB.Exec(t, `LOAD VIEW foov FROM "nodelocal:///shows";`)

	res := sqlDB.Query(t, `SELECT * FROM foov`)
	var str string
	for res.Next() {
		var a int
		var b int
		var c int
		var d int
		err := res.Scan(&a, &b, &c, &d)
		if err != nil {
			t.Fatal(err)
		}
		fmt.Println(a, b, c, d)
		str += fmt.Sprintf("%d,%d,%d,%d", a, b, c, d)
		str += "\n"
	}

	if expected, got := "1,12,4,14\n2,22,3,24\n3,32,2,34\n5,12,4,14\n", str; expected != got {
		t.Fatalf("expected %q, got %q", expected, got)
	}
}

func TestEncodingToGBKorUTF8(t *testing.T) {
	defer leaktest.AfterTest(t)()

	const numAccounts = 1000

	_, _, sqlDB, _, cleanupFn := dumpLoadTestSetup(t, singleNode, numAccounts, initNone)
	defer cleanupFn()
	const dir = "nodelocal:///"

	dumpTableGBK, dumpTableUTF8 := filepath.Join(dir, "dumpTableGBK"), filepath.Join(dir, "dumpTableUTF8")
	loadTableGBK, loadTableUTF8 := filepath.Join(dumpTableGBK, "n1.0.csv"), filepath.Join(dumpTableUTF8, "n1.0.csv")
	sqlDB.Exec(t, `CREATE TABLE data.t1(c1 int,name string)`)
	sqlDB.Exec(t, `INSERT INTO data.public.t1 VALUES (1,'浪潮'),(2,'TOM')`)

	//正确的预期结果
	expected := sqlDB.QueryStr(
		t, fmt.Sprintf(`SELECT * FROM data.t1`),
	)
	gbkInBinidb := "1,\xc0˳\xb1\n2,TOM\n"

	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP TO CSV $1 from table data.public.t1 with encoding = gbk;`),
		dumpTableGBK,
	)

	sqlDB.Exec(t,
		fmt.Sprintf(`DUMP TO CSV $1 from table data.public.t1 with encoding = utf8;`),
		dumpTableUTF8,
	)

	t.Run("dumpGBK", func(t *testing.T) {
		sqlDB.Exec(t,
			`LOAD TABLE data.public.gbk(id int, name string) csv data ($1)`,
			loadTableGBK,
		)
		res := sqlDB.Query(t, `SELECT * FROM data.public.gbk`)
		var str string
		for res.Next() {
			var a int
			var b string
			err := res.Scan(&a, &b)
			if err != nil {
				t.Fatal(err)
			}
			str += fmt.Sprintf("%d,%v", a, b)
			str += "\n"
		}
		if str != gbkInBinidb {
			t.Fatalf("expect %q, got %q", gbkInBinidb, str)
		}
	})

	//编码格式仅支持GBK和UTF-8，此处为测试指定错误时的报错
	t.Run("wrongFormat", func(t *testing.T) {
		sqlDB.ExpectErr(t,
			`pq: Unsupported encoding format: abc`,
			fmt.Sprintf(`DUMP TO CSV $1 from table data.public.t1 with encoding = abc;`),
			dumpTableGBK,
		)
	})

	//使用了with encoding却未指定值
	t.Run("nilEncoding", func(t *testing.T) {
		sqlDB.ExpectErr(t,
			`pq: option "encoding" requires a value`,
			`DUMP TO CSV $1 from table data.public.t1 with encoding;`,
			dumpTableGBK,
		)
	})

	t.Run("loadGBK", func(t *testing.T) {
		sqlDB.Exec(t,
			`LOAD TABLE data.public.s1(a int,b string) csv data ($1) with encoding = gbk;`,
			loadTableGBK,
		)
		sqlDB.CheckQueryResults(t, `SELECT * FROM data.s1`, expected)
	})

	t.Run("loadUTF8", func(t *testing.T) {
		sqlDB.Exec(t,
			`LOAD TABLE data.public.s2(a int, b string) csv data ($1) with encoding = utf8;`,
			loadTableUTF8,
		)
		sqlDB.CheckQueryResults(t, `SELECT * FROM data.s2`, expected)
	})
}

func TestLoadSequence(t *testing.T) {
	defer leaktest.AfterTest(t)()
	dir, cleanupDir := testutils.TempDir(t)
	defer cleanupDir()

	srv, db, _ := serverutils.StartServer(t, base.TestServerArgs{ExternalIODir: dir})
	defer srv.Stopper().Stop(context.Background())
	sqlDB := sqlutils.MakeSQLRunner(db)

	sqlDB.Exec(t, `Drop database if exists data cascade;`)
	sqlDB.Exec(t, `create database data; `)
	sqlDB.Exec(t, `create sequence  data.seq_test  increment by 1 start with 10  maxvalue 300   minvalue 5;`)
	sqlDB.Exec(t, `DUMP sequence data.seq_test TO SST "nodelocal:///seq";`)
	sqlDB.Exec(t, `drop sequence data.seq_test;`)
	sqlDB.Exec(t, `LOAD sequence data.seq_test FROM "nodelocal:///seq";`)

	res := sqlDB.Query(t, `show sequences from data`)
	var str string
	for res.Next() {
		var a string
		var b string
		var c string
		err := res.Scan(&a, &b, &c)
		if err != nil {
			t.Fatal(err)
		}
		fmt.Println(a, b)
		str += fmt.Sprintf("%s,%s,%s", a, b, c)
		str += "\n"
	}

	if expected, got := "public,seq_test,root\n", str; expected != got {
		t.Fatalf("expected %q, got %q", expected, got)
	}
}

func TestLoadMaterializedViewbyTables(t *testing.T) {
	defer leaktest.AfterTest(t)()
	dir, cleanupDir := testutils.TempDir(t)
	defer cleanupDir()

	srv, db, _ := serverutils.StartServer(t, base.TestServerArgs{ExternalIODir: dir})
	defer srv.Stopper().Stop(context.Background())
	sqlDB := sqlutils.MakeSQLRunner(db)

	sqlDB.Exec(t, `drop database if exists data cascade`)
	sqlDB.Exec(t, `create database data`)
	sqlDB.Exec(t, `CREATE TABLE  data.t1 ( i1 INT, s1 STRING )`)
	sqlDB.Exec(t, `CREATE TABLE  data.t2 ( i2 INT, s2 STRING )`)
	sqlDB.Exec(t, `CREATE MATERIALIZED VIEW  data.mt1 ( mi1, ms1 ) AS  SELECT  i1, s2 FROM data.t1, data.t2`)
	sqlDB.Exec(t, `DUMP  VIEW data.mt1 TO SST "nodelocal:///v1"`)
	sqlDB.Exec(t, `DROP MATERIALIZED view data.mt1`)
	sqlDB.Exec(t, `LOAD  VIEW data.mt1 FROM "nodelocal:///v1"`)

	res := sqlDB.Query(t, `show tables from data`)
	var str string
	for res.Next() {
		var a string
		var b string
		err := res.Scan(&a, &b)
		if err != nil {
			t.Fatal(err)
		}
		fmt.Println(a)
		str += fmt.Sprintf("%s,%s", a, b)
		str += "\n"
	}

	if expected, got := "mt1,root\nt1,root\nt2,root\n", str; expected != got {
		t.Fatalf("expected %q, got %q", expected, got)
	}
}

func TestLoadMaterializedViewbyViews(t *testing.T) {
	defer leaktest.AfterTest(t)()
	dir, cleanupDir := testutils.TempDir(t)
	defer cleanupDir()

	srv, db, _ := serverutils.StartServer(t, base.TestServerArgs{ExternalIODir: dir})
	defer srv.Stopper().Stop(context.Background())
	sqlDB := sqlutils.MakeSQLRunner(db)

	sqlDB.Exec(t, `drop database if exists data cascade;`)
	sqlDB.Exec(t, `create database data; `)
	sqlDB.Exec(t, `CREATE TABLE  data.t1 ( i1 INT, s1 STRING )`)
	sqlDB.Exec(t, `CREATE TABLE  data.t2 ( i2 INT, s2 STRING )`)
	sqlDB.Exec(t, `CREATE VIEW  data.vt1 ( vi1, vs1 ) AS  SELECT  i1, s1 FROM data.t1`)
	sqlDB.Exec(t, `CREATE VIEW  data.vt2 ( vi2, vs2 ) AS  SELECT  i2, s2 FROM data.t2`)
	sqlDB.Exec(t, `CREATE MATERIALIZED VIEW  data.mt1 ( mi2, ms2 ) AS  SELECT  vi2, vs1 FROM data.vt1, data.vt2`)
	sqlDB.Exec(t, `DUMP  VIEW data.mt1 TO SST "nodelocal:///v1";`)
	sqlDB.Exec(t, `DROP MATERIALIZED view data.mt1 ;`)
	sqlDB.Exec(t, `LOAD  VIEW data.mt1 FROM "nodelocal:///v1";`)

	res := sqlDB.Query(t, `show tables from data`)
	var str string
	for res.Next() {
		var a string
		var b string
		err := res.Scan(&a, &b)
		if err != nil {
			t.Fatal(err)
		}
		fmt.Println(a)
		str += fmt.Sprintf("%s,%s", a, b)
		str += "\n"
	}

	if expected, got := "mt1,root\nt1,root\nt2,root\nvt1,root\nvt2,root\n", str; expected != got {
		t.Fatalf("expected %q, got %q", expected, got)
	}
}

func TestLoadMaterializedViewbyMviews(t *testing.T) {
	defer leaktest.AfterTest(t)()
	dir, cleanupDir := testutils.TempDir(t)
	defer cleanupDir()

	srv, db, _ := serverutils.StartServer(t, base.TestServerArgs{ExternalIODir: dir})
	defer srv.Stopper().Stop(context.Background())
	sqlDB := sqlutils.MakeSQLRunner(db)

	sqlDB.Exec(t, `drop database if exists data cascade;`)
	sqlDB.Exec(t, `create database data`)
	sqlDB.Exec(t, `CREATE TABLE  data.t1 ( i1 INT, s1 STRING )`)
	sqlDB.Exec(t, `CREATE TABLE  data.t2 ( i2 INT, s2 STRING )`)
	sqlDB.Exec(t, `CREATE MATERIALIZED VIEW  data.mt1 ( mi1, ms1 ) AS  SELECT  i1, s2 FROM data.t1, data.t2`)
	sqlDB.Exec(t, `CREATE VIEW  data.vt1 ( vi1, vs1 ) AS  SELECT  i1, s1 FROM data.t1`)
	sqlDB.Exec(t, `CREATE VIEW  data.vt2 ( vi2, vs2 ) AS  SELECT  i2, s2 FROM data.t2`)
	sqlDB.Exec(t, `CREATE MATERIALIZED VIEW  data.mt2 ( mi2, ms2 ) AS  SELECT  vi2, vs1 FROM data.vt1, data.vt2`)
	sqlDB.Exec(t, `CREATE MATERIALIZED VIEW  data.mmt ( mmi, mms ) AS  SELECT  mi1, ms2 FROM data.mt1, data.mt2`)
	sqlDB.Exec(t, `DUMP  VIEW data.mmt TO SST "nodelocal:///v1"`)
	sqlDB.Exec(t, `DROP MATERIALIZED view data.mmt`)
	sqlDB.Exec(t, `LOAD  VIEW data.mmt FROM "nodelocal:///v1"`)

	res := sqlDB.Query(t, `show tables from data`)
	var str string
	for res.Next() {
		var a string
		var b string
		err := res.Scan(&a, &b)
		if err != nil {
			t.Fatal(err)
		}
		str += fmt.Sprintf("%s,%s", a, b)
		str += "\n"
	}

	if expected, got := "mmt,root\nmt1,root\nmt2,root\nt1,root\nt2,root\nvt1,root\nvt2,root\n", str; expected != got {
		t.Fatalf("expected %q, got %q", expected, got)
	}
}
